{
	"name": "parqueToScbAll",
	"properties": {
		"description": "This code converts the daily parquet file of Ananthapuram to SCBAll Table that will be further utilized to find the faulty SCBs and send notifications ",
		"folder": {
			"name": "Antpr"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "Sapientia",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "7ce3b0c9-242f-4642-b7bb-391877847b23"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/37a47312-6f04-42dc-ad99-290d950fab5d/resourceGroups/AzureSynapse/providers/Microsoft.Synapse/workspaces/sapience/bigDataPools/Sapientia",
				"name": "Sapientia",
				"type": "Spark",
				"endpoint": "https://sapience.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sapientia",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd \r\n",
					"import datetime\r\n",
					"import numpy as np\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import *\r\n",
					"import pyspark.sql.functions as sf\r\n",
					"from datetime import datetime \r\n",
					"from datetime import date \r\n",
					"from datetime import timedelta\r\n",
					"# from pyspark.sql.functions import to_date\r\n",
					"from datetime import time\r\n",
					"import pytz\r\n",
					"from pathlib import Path\r\n",
					"import pymsteams"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pattoreadparquet = \"abfss://repono@ayanadatalake.dfs.core.windows.net/allcsvtoparquet/dt=\"\r\n",
					"LoopCounter=1\r\n",
					"tz = pytz.timezone('Asia/Kolkata')\r\n",
					"todaydate = datetime.date(datetime.now(tz))\r\n",
					"print(todaydate)\r\n",
					"mintime = datetime.min.time()\r\n",
					"DateList =[]\r\n",
					"while LoopCounter <= 1 :\r\n",
					"    DateList.append( datetime.date ( (datetime.combine(todaydate, mintime) - timedelta(days=LoopCounter)) ))\r\n",
					"    LoopCounter += 1\r\n",
					"DateListLength = len(DateList)\r\n",
					"print(DateList)\r\n",
					"TableLoopCounter = 0\r\n",
					"DateLoopCounter = 0\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"location1 = pattoreadparquet   \r\n",
					"location1= location1+ str(DateList[DateLoopCounter])+str(\"/\")\r\n",
					"print(location1)\r\n",
					"df = pd.read_parquet(location1)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"New=pd.DataFrame(columns=[])\r\n",
					"Block_df=pd.DataFrame(columns=[])\r\n",
					"Cloud=pd.DataFrame(columns=[])\r\n",
					"\r\n",
					"dict={'ICR08':'Antpr1','ICR10':'Antpr1','ICR12':'Antpr1','ICR13':'Antpr1','ICR16':'Antpr2',\r\n",
					"      'ICR17':'Antpr2','ICR18':'Antpr2','ICR19':'Antpr2','ICR01':'Antpr3','ICR02':'Antpr3',\r\n",
					"      'ICR03':'Antpr3','ICR04':'Antpr3','ICR11':'Antpr4','ICR14':'Antpr4','ICR15':'Antpr4',\r\n",
					"      'ICR20':'Antpr4','ICR05':'Antpr5','ICR06':'Antpr5','ICR07':'Antpr5','ICR09':'Antpr5'}\r\n",
					"dict1={'B01':'Antpr1','B02':'Antpr2','B03':'Antpr3','B04':'Antpr4','B05':'Antpr5'}\r\n",
					"block=['Antpr1','Antpr2','Antpr3','Antpr4','Antpr5']\r\n",
					"Block_Name=['Block1','Block2','Block3','Block4','Block5']\r\n",
					"SMB_Analysis=['B1_SMBAnalysis','B2_SMBAnalysis','B3_SMBAnalysis','B4_SMBAnalysis','B5_SMBAnalysis']\r\n",
					"Output_File=['B1_SMB_Current_Antpr','B2_SMB_Current_Antpr','B3_SMB_Current_Antpr','B4_SMB_Current_Antpr','B5_SMB_Current_Antpr']\r\n",
					"Output_File1=['B1_SMB_Current_All_Antpr','B2_SMB_Current_All_Antpr','B3_SMB_Current_All_Antpr','B4_SMB_Current_All_Antpr','B5_SMB_Current_All_Antpr']\r\n",
					"Cloud_Output=['B1_Cloud_Data_Antpr','B2_Cloud_Data_Antpr','B3_Cloud_Data_Antpr','B4_Cloud_Data_Antpr','B5_Cloud_Data_Antpr']\r\n",
					"Critic_Output=['B1_Critical_SMB_Antpr','B2_Critical_SMB_Antpr','B3_Critical_SMB_Antpr','B4_Critical_SMB_Antpr','B5_Critical_SMB_Antpr']\r\n",
					"\r\n",
					"location_input=\"abfss://repono@ayanadatalake.dfs.core.windows.net/staticfiles/\"\r\n",
					"location_output=\"abfss://repono@ayanadatalake.dfs.core.windows.net/scbhisraw/SCBFaults/\"\r\n",
					"df = pd.read_parquet(location1)\r\n",
					"#df = pd.read_parquet(\"abfss://repono@ayanadatalake.dfs.core.windows.net/allcsvtoparquet/dt=2022-01-31/\") #USE THIS LIKE FOR RUNNING FOR ANYPARTICULAR DAY\r\n",
					"\r\n",
					"\r\n",
					"tags=pd.read_csv(location_input+\"OperationalTagsofSCBs.csv\")\r\n",
					"\r\n",
					"tag=list(tags['Tags'].dropna())\r\n",
					"\r\n",
					"df=df[df['itemname'].isin(tag)] \r\n",
					"\r\n",
					"df['Time']= df['ISTtime'].dt.strftime(\"%H:%M\")\r\n",
					"df=df[(df['Time'] >='06:00') & (df['Time']<='19:00')]\r\n",
					"\r\n",
					"New['Date_And_Time'] = pd.to_datetime(df['ISTtime']).dt.strftime(\"%Y-%m-%d %H:%M\")\r\n",
					"New['Date']= df['ISTtime'].dt.strftime(\"%Y-%m-%d\")\r\n",
					"New['Time']= df['ISTtime'].dt.strftime(\"%H:%M\")\r\n",
					"New['Sitename']=df['sitename']\r\n",
					"New['Tags']=df['itemname']\r\n",
					"New['Value']=df['value']\r\n",
					"New.reset_index(level=0, inplace=True)   ### Resetting  Index \r\n",
					"del New['index']                                           \r\n",
					"df.reset_index(level=0, inplace=True)    \r\n",
					"del df['index']                                           \r\n",
					"Columns=New.Tags.str.split('.',expand=True)\r\n",
					"Frame=[New,Columns]\r\n",
					"New=pd.concat(Frame,axis=1)\r\n",
					"del Columns\r\n",
					"del Frame\r\n",
					"New=New.rename(columns ={0:\"ICR\"})\r\n",
					"New=New.rename(columns ={2:\"INV\"})  \r\n",
					"New=New.rename(columns ={3:\"SCB\"})\r\n",
					"New['Number']=New['ICR'].str.extract('(\\d+)')\r\n",
					"New['Number']=New['Number'].str.zfill(2)\r\n",
					"New['Number'] = New['Number'].replace(np.nan, '', regex=True)\r\n",
					"New['Number'] = New['Number'].replace([None], [''], regex=True)\r\n",
					"New['Char']=(New['ICR'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"New['Char'] = New['Char'].replace(np.nan,'', regex=True)\r\n",
					"New['Char'] = New['Char'].replace([None],[''], regex=True)\r\n",
					"New['ICR']=New['Char']+New['Number']\r\n",
					"New['Number']=New['INV'].str.extract('(\\d+)')\r\n",
					"New['Number']=New['Number'].str.zfill(2)\r\n",
					"New['Number'] = New['Number'].replace(np.nan, '', regex=True)\r\n",
					"New['Number'] = New['Number'].replace([None],[''], regex=True)\r\n",
					"New['Char']=(New['INV'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"New['Char'] = New['Char'].replace(np.nan, '', regex=True)\r\n",
					"New['Char'] = New['Char'].replace([None], [''], regex=True)\r\n",
					"New['INV']=New['Char']+New['Number']\r\n",
					"New['Number']=New['SCB'].str.extract('(\\d+)')\r\n",
					"New['Number']=New['Number'].str.zfill(2)\r\n",
					"New['Number'] = New['Number'].replace(np.nan, '', regex=True)\r\n",
					"New['Number'] = New['Number'].replace([None], [''], regex=True)\r\n",
					"New['Char']=(New['SCB'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"New['Char'] = New['Char'].replace(np.nan, '', regex=True)\r\n",
					"New['Char'] = New['Char'].replace([None], [''], regex=True)\r\n",
					"New['SCB']=New['Char']+New['Number']\r\n",
					"del New['Char'],New['Number']\r\n",
					"\r\n",
					"New['ICR']=New['ICR'].replace([None],[''] , regex=True)\r\n",
					"New['INV']=New['INV'].replace([''],[None] , regex=True)\r\n",
					"New['SCB']=New['SCB'].replace([''],[None] , regex=True)\r\n",
					"\r\n",
					"New['Block1'] = New['ICR'].map(dict)\r\n",
					"New['Block1'] = New['Block1'].replace(np.nan, '', regex=True)\r\n",
					"New['Block2'] = New['INV'].map(dict1)\r\n",
					"New['Block2'] = New['Block2'].replace(np.nan, '', regex=True)\r\n",
					"New['Block']=New['Block1']+New['Block2']\r\n",
					"del New['Block1'],New['Block2']\r\n",
					"\r\n",
					"New['ICR'] = New['ICR'].replace(np.nan, '', regex=True)\r\n",
					"New['ICR'] = New['ICR'].replace([None], [''], regex=True)\r\n",
					"New['INV'] = New['INV'].replace(np.nan, '', regex=True)\r\n",
					"New['INV'] = New['INV'].replace([None], [''], regex=True)\r\n",
					"New['SCB'] = New['SCB'].replace(np.nan, '', regex=True)\r\n",
					"New['SCB'] = New['SCB'].replace([None], [''], regex=True)\r\n",
					"New['Block'] = New['Block'].replace(np.nan, '', regex=True)\r\n",
					"New['Block'] = New['Block'].replace([None], [''], regex=True)\r\n",
					"\r\n",
					"New = New.groupby([\"Date_And_Time\",\"Date\",\"Time\",\"Sitename\",\"Block\",\"Tags\",\"ICR\",\"INV\",\"SCB\"]).Value.sum().reset_index()\r\n",
					"New_df = pd.to_timedelta(New['Time']+':00').sub(pd.to_timedelta('1min'))\r\n",
					"New_df=(New\r\n",
					" .groupby([New_df.dt.floor('5min'), \"Tags\"]) \r\n",
					" .agg({'Value': 'sum','Date':'last','Sitename':'last','Block':'last','ICR':'last','INV':'last','SCB':'last'})\r\n",
					" .reset_index()\r\n",
					" .assign(Time=lambda d: (pd.to_datetime(0)+d['Time']).dt.strftime('%H:%M')))\r\n",
					"New_df['Date_And_Time']=pd.to_datetime(New_df['Date'] +' '+ New_df['Time']).dt.strftime('%Y-%m-%d %H:%M')\r\n",
					"New_df = New_df.reindex(columns=['Date_And_Time','Date','Time','Sitename','Block','Tags','ICR','INV','SCB','Value'])\r\n",
					"New_df.to_csv(location_output+\"scbraw.csv\",index=False)\r\n",
					"Temp_df=New_df\r\n",
					"Columns=['Date','Time','Tags','Value']\r\n",
					"New_df= pd.DataFrame(New_df, columns=Columns)\r\n",
					"Date=New_df.iloc[0,0]\r\n",
					"New_df1=New_df\r\n",
					"New_df1=pd.pivot(\r\n",
					"    data=New_df1,        \r\n",
					"    index='Time',    # Column to use to make new frame’s index. If None, uses existing index.\r\n",
					"    columns='Tags',  # Column to use to make new frame’s columns.\r\n",
					"    values='Value'    # Column(s) to use for populating new frame’s values.\r\n",
					")\r\n",
					"New_df1['Date']=Date\r\n",
					"tag.insert(0,'Date')\r\n",
					"New_df1 = New_df1.reindex(columns=tag)\r\n",
					"for indx in range(5):\r\n",
					"    KPI=''\r\n",
					"    Not_Working=[]\r\n",
					"    print(block[indx])\r\n",
					"    SMB_Analysis_File=pd.read_csv(location_input+str(SMB_Analysis[indx])+\".csv\")\r\n",
					"    cols=[]\r\n",
					"    Block_df=pd.DataFrame(columns=cols)\r\n",
					"    Block_df1=pd.DataFrame(columns=cols)\r\n",
					"    Block_List=pd.DataFrame(tags[Block_Name[indx]].dropna())\r\n",
					"    Block_List=Block_List.rename(columns ={Block_Name[indx]:\"Tags\"})\r\n",
					"    Block_df= pd.DataFrame(New_df1, columns=Block_List['Tags'])\r\n",
					"    Block_df.reset_index(level=0, inplace=True)    \r\n",
					"    Block_df=Block_df.rename(columns ={Block_List.loc[1,'Tags']:\"GIR\"})\r\n",
					"    Not_Working=(Block_df.columns[Block_df.isna().all(0)])\r\n",
					"    Block_df=Block_df.drop(columns=Not_Working,axis=1)            ### Dropping the unwanted columns\r\n",
					"    check_for_nan = Block_df.loc[:, Block_df.isna().any()]\r\n",
					"    check_for_nan=list(check_for_nan.columns)\r\n",
					"    if 'GIR' in check_for_nan: \r\n",
					"        check_for_nan.remove('GIR')\r\n",
					"    Block_df=Block_df.drop(columns=check_for_nan,axis=1)            ### Dropping the unwanted columns\r\n",
					"    Block_df=Block_df.replace(np.nan, 10)\r\n",
					"    Block_List=Block_List.drop([0,1],axis=0)            ### Dropping the unwanted columns\r\n",
					"    index_number=Block_List[Block_List['Tags'].isin(Not_Working)].index\r\n",
					"    Block_List=Block_List.drop(index_number,axis=0)            ### Dropping the unwanted rows\r\n",
					"    Block_List.reset_index(level=0, inplace=True)\r\n",
					"    del Block_List['index']\r\n",
					"    index_number=Block_List[Block_List['Tags'].isin(check_for_nan)].index\r\n",
					"    Block_List=Block_List.drop(index_number,axis=0)            ### Dropping the unwanted rows\r\n",
					"    Block_List.reset_index(level=0, inplace=True)  \r\n",
					"    del Block_List['index']\r\n",
					"    index_number=SMB_Analysis_File[SMB_Analysis_File['Tags'].isin(Not_Working)].index\r\n",
					"    SMB_Analysis_File=SMB_Analysis_File.drop(index_number,axis=0)            ### Dropping the unwanted rows\r\n",
					"    SMB_Analysis_File.reset_index(level=0, inplace=True)\r\n",
					"    del SMB_Analysis_File['index']\r\n",
					"    index_number=SMB_Analysis_File[SMB_Analysis_File['Tags'].isin(check_for_nan)].index\r\n",
					"    SMB_Analysis_File=SMB_Analysis_File.drop(index_number,axis=0)            ### Dropping the unwanted rows\r\n",
					"    SMB_Analysis_File.reset_index(level=0, inplace=True)\r\n",
					"    del SMB_Analysis_File['index']\r\n",
					"    Columns=Block_List.Tags.str.split('.',expand=True)\r\n",
					"    Frame=[Block_df1,Columns]\r\n",
					"    Block_df1=pd.concat(Frame,axis=1)\r\n",
					"    Block_df1=Block_df1.rename(columns ={0:\"ICR\"})\r\n",
					"    Block_df1=Block_df1.rename(columns ={2:\"INV\"})  \r\n",
					"    Block_df1=Block_df1.rename(columns ={3:\"SCB\"})\r\n",
					"    del Block_df1[1]\r\n",
					"    Block_df1['Number']=Block_df1['ICR'].str.extract('(\\d+)')\r\n",
					"    Block_df1['Number']=Block_df1['Number'].str.zfill(2)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['Char']=(Block_df1['ICR'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace(np.nan,'', regex=True)\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace([None],[''], regex=True)\r\n",
					"    Block_df1['ICR']=Block_df1['Char']+Block_df1['Number']\r\n",
					"    Block_df1['Number']=Block_df1['INV'].str.extract('(\\d+)')\r\n",
					"    Block_df1['Number']=Block_df1['Number'].str.zfill(2)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace([None],[''], regex=True)\r\n",
					"    Block_df1['Char']=(Block_df1['INV'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['INV']=Block_df1['Char']+Block_df1['Number']\r\n",
					"    Block_df1['Number']=Block_df1['SCB'].str.extract('(\\d+)')\r\n",
					"    Block_df1['Number']=Block_df1['Number'].str.zfill(2)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['Char']=(Block_df1['SCB'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['SCB']=Block_df1['Char']+Block_df1['Number']\r\n",
					"    del Block_df1['Char'],Block_df1['Number']\r\n",
					"    \r\n",
					"    Block_df1['ICR']=Block_df1['ICR'].replace([None],[''] , regex=True)\r\n",
					"    Block_df1['INV']=Block_df1['INV'].replace([''],[None] , regex=True)\r\n",
					"    Block_df1['SCB']=Block_df1['SCB'].replace([''],[None] , regex=True)\r\n",
					"    \r\n",
					"    Block_df1['Block1'] = Block_df1['ICR'].map(dict)\r\n",
					"    Block_df1['Block1'] = Block_df1['Block1'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Block2'] = Block_df1['INV'].map(dict1)\r\n",
					"    Block_df1['Block2'] = Block_df1['Block2'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Block']=Block_df1['Block1']+Block_df1['Block2']\r\n",
					"    del Block_df1['Block1'],Block_df1['Block2']\r\n",
					"    \r\n",
					"    Block_df1['ICR'] = Block_df1['ICR'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['ICR'] = Block_df1['ICR'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['INV'] = Block_df1['INV'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['INV'] = Block_df1['INV'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['SCB'] = Block_df1['SCB'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['SCB'] = Block_df1['SCB'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['Block'] = Block_df1['Block'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Block'] = Block_df1['Block'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['Date']=Date\r\n",
					"    Icr_Inv_SCB=list(Block_df1['ICR']+'.'+Block_df1['INV']+'.'+Block_df1['SCB'])\r\n",
					"    Old_Coulmn_Name=Block_df.columns\r\n",
					"    Appending_List=['Time','Date','GIR']\r\n",
					"    Icr_Inv_SCB_1=Appending_List+Icr_Inv_SCB\r\n",
					"    Block_df.rename(columns={i:j for i,j in zip(Old_Coulmn_Name,Icr_Inv_SCB_1)}, inplace=True)\r\n",
					"    Block_df = Block_df.sort_index(ascending=True, axis=1)\r\n",
					"    Time_List=Block_df['Time']\r\n",
					"    del Block_df['Time']\r\n",
					"    Block_df.insert(1, \"Time\", Time_List)\r\n",
					"    Block_df1['Block_Inverter_SCB']=Icr_Inv_SCB\r\n",
					"    Block_df1 = Block_df1.sort_values(by = 'Block_Inverter_SCB')\r\n",
					"    index_number=Block_df1[Block_df1['Block_Inverter_SCB'].isin(Not_Working)].index\r\n",
					"    Block_df1=Block_df1.drop(index_number,axis=0)          ### Dropping the unwanted columns\r\n",
					"    Block_df1.reset_index(level=0, inplace=True)  \r\n",
					"    del Block_df1['index']\r\n",
					"    index_number=Block_df1[Block_df1['Block_Inverter_SCB'].isin(check_for_nan)].index\r\n",
					"    Block_df1=Block_df1.drop(index_number,axis=0)          ### Dropping the unwanted columns\r\n",
					"    Block_df1.reset_index(level=0, inplace=True)  \r\n",
					"    del Block_df1['index']\r\n",
					"    GIR_List=list(Block_df['GIR'])\r\n",
					"    Sum=list(Block_df.sum())\r\n",
					"    Sum.pop(0)      ### Popping the sum of the column Date,Time and GIR value from total Sum\r\n",
					"    Sum.pop(0)\r\n",
					"    Sum.pop(0)\r\n",
					"    Block_df1['Total_SCB_Current']=Sum\r\n",
					"    Block_df1['Deviation_wrt_R1_Block_df1']=(Block_df1['Total_SCB_Current']-(Block_df1['Total_SCB_Current'].max()))/(Block_df1['Total_SCB_Current'].max())\r\n",
					"    Curr_Per_String=Sum/SMB_Analysis_File['Effective_Strings']       \r\n",
					"    a = np.array(GIR_List)\r\n",
					"    Max_Value_Index = np.argmax(a)\r\n",
					"    Max_Value=GIR_List[Max_Value_Index]\r\n",
					"    def calculate_rank(Curr_Per_String):                                    ### This \"for\" loop is used to calculate the rank based on Curr_Per_String & stores it in a list & returns\r\n",
					"        a={}\r\n",
					"        rank=1\r\n",
					"        for num in sorted(Curr_Per_String, reverse=True):\r\n",
					"            if num not in a:\r\n",
					"                a[num]=rank\r\n",
					"                rank=rank+1\r\n",
					"        return[a[i] for i in  Curr_Per_String]\r\n",
					"    Rank1=calculate_rank(Curr_Per_String)   \r\n",
					"    Block_df1['Eff_String']=SMB_Analysis_File['Effective_Strings']\r\n",
					"    Block_df1[\"Total_String_Current\"]=Block_df1['Total_SCB_Current']/Block_df1['Eff_String']\r\n",
					"    Block_df1['Deviation_All']=(Block_df1[\"Total_String_Current\"]-Block_df1[\"Total_String_Current\"].max())/Block_df1[\"Total_String_Current\"].max()\r\n",
					"    Block_df1['OverAll_Block_df1_Eff_Rank']=Rank1\r\n",
					"    Block_df1['Ideal_Current']=SMB_Analysis_File['Ideal_Current']\r\n",
					"    Block_df1['Current_At_Max_GIR']=list(Block_df.iloc[Max_Value_Index,3:])\r\n",
					"    Block_df1['Ideal_Current_At_Max_GIR']=(Block_df1['Ideal_Current']/1000)*Max_Value\r\n",
					"    Block_df1['Delta']=Block_df1['Current_At_Max_GIR']-Block_df1['Ideal_Current_At_Max_GIR']\r\n",
					"    Block_df1['Rank_Delta']=Block_df1.groupby('Date')['Delta'].rank(ascending=False)\r\n",
					"    Block_df1['Rank_Day_Peak']=Block_df1['Rank_Delta']*Block_df1['OverAll_Block_df1_Eff_Rank']   \r\n",
					"    \r\n",
					"    Rank2=[]\r\n",
					"    rank_list=Block_df1['Rank_Day_Peak']\r\n",
					"    def calculate_rank1(rank_list): ### This \"for\" loop is used to calculate the rank based on sum_of_cols & stores it in a list & returns\r\n",
					"        a={}\r\n",
					"        rank=1\r\n",
					"        for num in sorted(rank_list, reverse=False):\r\n",
					"            if num not in a:\r\n",
					"                a[num]=rank\r\n",
					"                rank=rank+1\r\n",
					"        return[a[i] for i in  rank_list]\r\n",
					"    Rank2=calculate_rank1(rank_list)\r\n",
					"    Block_df1['Rank_Ranked_Day_Peak']=Rank2\r\n",
					"    Block_df1.to_csv(location_output+str(Output_File[indx])+\".csv\",index=False)\r\n",
					"    Block_df.to_csv(location_output+str(Output_File1[indx])+\".csv\",index=False)\r\n",
					"\r\n",
					"    \r\n",
					"    \r\n",
					"    df_spark2=spark.createDataFrame(Block_df1)\r\n",
					"    spark.sql(\"create database if not exists reassets\")\r\n",
					"    df_spark2.write.mode(\"append\").saveAsTable(\"reassets.scbperf\")\r\n",
					"\r\n",
					"    \r\n",
					"    \r\n",
					"    Dup=Block_df1\r\n",
					"    Dup.sort_values(\"Rank_Ranked_Day_Peak\", axis = 0, ascending = False,inplace = True)\r\n",
					"    Dup.reset_index(level=0, inplace=True)\r\n",
					"    Comparision=pd.read_csv(location_input+\"Check.csv\")\r\n",
					"    Comparision_Per=(Comparision.loc[0,'Percentage_of_Critical_SMB'])/100\r\n",
					"    High_Value=Dup.loc[0,'Rank_Ranked_Day_Peak']\r\n",
					"    Low_Value=Dup.loc[Dup['Rank_Ranked_Day_Peak'].count()-1,'Rank_Ranked_Day_Peak']\r\n",
					"    RESULT=High_Value-Low_Value\r\n",
					"    RESULT=RESULT*Comparision_Per\r\n",
					"    Final_Val=High_Value-RESULT\r\n",
					"    \r\n",
					"    Lossing=[]\r\n",
					"    Loss_str=[]\r\n",
					"    for g in range(Block_df1['Rank_Ranked_Day_Peak'].count()):\r\n",
					"        if Block_df1.loc[g,'Rank_Ranked_Day_Peak']>Final_Val:\r\n",
					"            Lossing.append(Block_df1.loc[g,'Block_Inverter_SCB'])\r\n",
					"            Loss_str+=str(Block_df1.loc[g,'Block_Inverter_SCB'])+'\\n\\n'\r\n",
					"    if Path(location_output+str(Critic_Output[indx])+'.csv').is_file():\r\n",
					"        Critic=pd.read_csv(location_output+str(Critic_Output[indx])+\".csv\")         ### If File is present, reading the that file into a dataframe\r\n",
					"    else:\r\n",
					"        cols=['Date','Critical_SMB']\r\n",
					"        Critic= pd.DataFrame(columns = cols)                             ### If File is not present, Create a new dataframe by above columns\r\n",
					"    for c in range(len(Lossing)):\r\n",
					"        Critic.loc[len(Critic)]=[Date,Lossing[c]]\r\n",
					"    Critic.to_csv(location_output+str(Critic_Output[indx])+'.csv',index=False)\r\n",
					"\r\n",
					"    \r\n",
					"                            ####################### Cloud Computing #########################\r\n",
					"    Rad=[]\r\n",
					"    Rad1=[]\r\n",
					"    Std_gir=[]\r\n",
					"    for j in range(Block_df['GIR'].count()):\r\n",
					"        Rad.append(Block_df.loc[j,'GIR'])\r\n",
					"        if(Rad[j]==0):\r\n",
					"            Rad[j]=1\r\n",
					"    row_no1=0\r\n",
					"    row_no2=0\r\n",
					"    for i in range(len(Rad)):\r\n",
					"        if Rad[i]>=820:\r\n",
					"            row_no1=i\r\n",
					"            break;\r\n",
					"    for i in reversed(Rad):\r\n",
					"        Rad1.append(i)\r\n",
					"    for i in range(len(Rad1)):\r\n",
					"        if Rad1[i]>=820:\r\n",
					"            row_no2=(len(Rad1)-i)-2\r\n",
					"            break;\r\n",
					"    Cloud['Date']=Block_df['Date']\r\n",
					"    Cloud['Time']=Block_df['Time']\r\n",
					"    Cloud['GIR']=Block_df['GIR']\r\n",
					"    for ind in range(Block_df['GIR'].count()):\r\n",
					"        Std_gir.append((Block_df.loc[ind,'GIR']-Block_df['GIR'].mean())/Block_df['GIR'].std())\r\n",
					"    inv1=1\r\n",
					"    blk1=1\r\n",
					"    smb1=1\r\n",
					"    qwer=0\r\n",
					"    cld=[0]\r\n",
					"    for index in range(len(Block_df.count())):   \r\n",
					"        if(index>2):\r\n",
					"            qwer+=1\r\n",
					"            std_curr=[]\r\n",
					"            curr=[]\r\n",
					"            Rad1=[]\r\n",
					"            curr1=[]\r\n",
					"            Rad2=[]\r\n",
					"            curr2=[]\r\n",
					"            Delta=[]\r\n",
					"            cld1=[]\r\n",
					"            cld2=[]\r\n",
					"            cld3=[]\r\n",
					"            cld4=[]\r\n",
					"            cld5=[]  \r\n",
					"            for oi in range(Block_df['GIR'].count()):    \r\n",
					"                std_curr.append((Block_df.iloc[oi,index]-Block_df.iloc[:,index].mean())/Block_df.iloc[:,index].std())\r\n",
					"                curr.append(Block_df.iloc[oi,index])\r\n",
					"                if(curr[oi]==0):\r\n",
					"                    curr[oi]=1\r\n",
					"            for io in range(len(Rad)-1):\r\n",
					"                Rad1.append(((Rad[io+1]-Rad[io])/Rad[io+1])*100)\r\n",
					"                curr1.append(((curr[io+1]-curr[io])/curr[io+1])*100)\r\n",
					"                Delta.append(curr1[io]-Rad1[io])\r\n",
					"            for io in range(len(Delta)):\r\n",
					"                if Delta[io]<Comparision.loc[0,'Percentage_of_Cloud_Difference']: \r\n",
					"                    cld1.append(1)\r\n",
					"                else:\r\n",
					"                    cld1.append(0)\r\n",
					"                Rad2.append((Rad[io]-Rad[io+1]))\r\n",
					"                curr2.append((curr[io]-curr[io+1]))\r\n",
					"            if row_no2!=0:\r\n",
					"                for iop in range(row_no2+1):\r\n",
					"                    if(Rad2[iop]>0 and curr2[iop]>0): \r\n",
					"                        cld2.append(1)\r\n",
					"                    else:\r\n",
					"                        cld2.append(0)\r\n",
					"                for poi in range(len(Rad2)):\r\n",
					"                    if poi>row_no2:\r\n",
					"                        if(Rad2[poi]<0 and curr2[poi]<0):\r\n",
					"                            cld2.append(1)\r\n",
					"                        else:\r\n",
					"                            cld2.append(0)\r\n",
					"            else:\r\n",
					"                 for iop in range(len(Rad2)):\r\n",
					"                    if(Rad2[iop]>0 and curr2[iop]>0): \r\n",
					"                        cld2.append(1)\r\n",
					"                    else:\r\n",
					"                        cld2.append(0)\r\n",
					"            for uio in range(len(cld1)):\r\n",
					"                if(cld1[uio]==1 or cld2[uio]==1):\r\n",
					"                    cld3.append(1)\r\n",
					"                else:\r\n",
					"                    cld3.append(0)\r\n",
					"            if row_no2!=0:\r\n",
					"                for oiu in range(len(cld3)):\r\n",
					"                    if oiu>=row_no1 and oiu<=row_no2:\r\n",
					"                        if Std_gir[oiu]<Std_gir[row_no1] and std_curr[oiu]<std_curr[row_no1]:\r\n",
					"                            cld4.append(1)\r\n",
					"                        else:\r\n",
					"                            cld4.append(0)\r\n",
					"                    else:\r\n",
					"                        cld4.append(0)               \r\n",
					"                cld4.pop(0)\r\n",
					"                cld4=cld4+cld\r\n",
					"            if row_no2!=0:           \r\n",
					"                for iu in range(len(cld3)):\r\n",
					"                    if(cld3[iu]==1 or cld4[iu]==1):\r\n",
					"                        cld5.append(1)\r\n",
					"                    else:\r\n",
					"                        cld5.append(0)\r\n",
					"            else:\r\n",
					"                cld5=cld3\r\n",
					"            str123=Block_df.columns[index]\r\n",
					"            cld5=cld+cld5\r\n",
					"            Cloud[str123]=cld5\r\n",
					"    Cloud.to_csv(location_output+str(Cloud_Output[indx])+\".csv\",index=False)\r\n",
					"    Min_row=Max_Value_Index-6\r\n",
					"    Max_row=Max_Value_Index+6\r\n",
					"    Sum1=[]\r\n",
					"    for c in range(len(Lossing)):\r\n",
					"        Total=0\r\n",
					"        for ind in range(Min_row,Max_row+1):\r\n",
					"            Total+=Cloud.loc[ind,Lossing[c]]\r\n",
					"        Sum1.append(Total)\r\n",
					"    a = np.array(Sum1)\r\n",
					"    Min_Value_Index = np.argmin(a)\r\n",
					"    Min_Value=Sum1[Min_Value_Index]\r\n",
					"    Loss_Str=''\r\n",
					"    for i in range(len(Lossing)):\r\n",
					"        if i==0 and Min_Value_Index==0:\r\n",
					"            Loss_Str+=str(Lossing[i])+u\"\\u26A0\"+''+u\"\\u26C5\"+\"\\n\\n\"\r\n",
					"        elif i==Min_Value_Index:\r\n",
					"            Loss_Str+=str(Lossing[i])+u\"\\u26C5\"+\"\\n\\n\"\r\n",
					"        elif i==0:\r\n",
					"            Loss_Str+=str(Lossing[i])+u\"\\u26A0\"+\"\\n\\n\"                                         \r\n",
					"        else:\r\n",
					"            Loss_Str+=str(Lossing[i])+\"\\n\\n\"\r\n",
					"    # print(Loss_Str)\r\n",
					"    comm_error=''\r\n",
					"    if len(Not_Working)>0 or len(check_for_nan)>0 :\r\n",
					"        comm_error='Communication Error:\\n\\n'\r\n",
					"        for comm in range(len(Not_Working)):\r\n",
					"            comm_error+=''.join(Not_Working[comm])+'\\n\\n'\r\n",
					"        for comm in range(len(check_for_nan)):\r\n",
					"            comm_error+=''.join(check_for_nan[comm])+'  $\\n\\n'\r\n",
					"        KPI='***'+block[indx]+': '+Date+'***\\n\\n'+comm_error+'Faulty SMBs:\\n\\n'+Loss_Str+'\\n\\n'\r\n",
					"    else:\r\n",
					"        KPI='***'+block[indx]+': '+Date+'***\\n\\nFaulty SMBs:\\n\\n'+Loss_Str+'\\n\\n'\r\n",
					"    def sms():\r\n",
					"        import pymsteams\r\n",
					"        #myTeamsMessage = pymsteams.connectorcard(\"https://ayanapower.webhook.office.com/webhookb2/ac6eaf82-b2d0-4b46-8e52-a17f3fc5051d@59b60474-e282-44b5-881c-bb9ce815690c/IncomingWebhook/ab1320c704ed43e88d7a1fe9b7618ee9/0a64ab19-2984-4746-9447-21882231bb32\")\r\n",
					"        myTeamsMessage = pymsteams.connectorcard(\"https://ayanapower.webhook.office.com/webhookb2/1f18b9b8-1eae-4197-b8b7-b8f730682bb1@59b60474-e282-44b5-881c-bb9ce815690c/IncomingWebhook/b20080145f584f28b7d12f94eb46f4b7/0a64ab19-2984-4746-9447-21882231bb32\")\r\n",
					"        myTeamsMessage.text(KPI)\r\n",
					"        myTeamsMessage.send()\r\n",
					"    sms()\r\n",
					"    #print(KPI)"
				],
				"execution_count": 4
			}
		]
	}
}