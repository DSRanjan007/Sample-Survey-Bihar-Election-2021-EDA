{
	"name": "PH_SCBs",
	"properties": {
		"description": "This code is written to read daily Parquet files and perform the transformation to create Table for Inverters' Performance for Phelan SPP",
		"folder": {
			"name": "Phelan"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "Sapientia",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "6fc3adec-185a-4abb-ba1d-131ff4d37d75"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/37a47312-6f04-42dc-ad99-290d950fab5d/resourceGroups/AzureSynapse/providers/Microsoft.Synapse/workspaces/sapience/bigDataPools/Sapientia",
				"name": "Sapientia",
				"type": "Spark",
				"endpoint": "https://sapience.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sapientia",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd \r\n",
					"import datetime\r\n",
					"import numpy as np\r\n",
					"\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import *\r\n",
					"import pyspark.sql.functions as sf\r\n",
					"from datetime import datetime \r\n",
					"from datetime import date \r\n",
					"from datetime import timedelta\r\n",
					"# from pyspark.sql.functions import to_date\r\n",
					"from datetime import time\r\n",
					"import pytz\r\n",
					"from pathlib import Path\r\n",
					"import pymsteams"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pattoreadcsv = \"abfss://phelanrealtime@ayanadatalake.dfs.core.windows.net/phelan_realtime/\"\r\n",
					"LoopCounter=1\r\n",
					"tz = pytz.timezone('Asia/Kolkata')\r\n",
					"todaydate = datetime.date(datetime.now(tz))\r\n",
					"print(todaydate)\r\n",
					"mintime = datetime.min.time()\r\n",
					"DateList =[]\r\n",
					"while LoopCounter <= 1 :\r\n",
					"    DateList.append( datetime.date ( (datetime.combine(todaydate, mintime) - timedelta(days=LoopCounter)) ))\r\n",
					"    LoopCounter += 1\r\n",
					"DateListLength = len(DateList)\r\n",
					"print(DateList)\r\n",
					"TableLoopCounter = 0\r\n",
					"DateLoopCounter = 0"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"location1 = pattoreadcsv   \r\n",
					"location1= location1+ str(DateList[DateLoopCounter]).split(\"-\")[0]+\"/\"+str(DateList[DateLoopCounter]).split(\"-\")[1]+\"/\"+str(DateList[DateLoopCounter]).split(\"-\")[2]+\"/\"+str(\"*\")\r\n",
					"print(location1)\r\n",
					"#df = pd.read_parquet(location1)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#df = spark.read.load(\"abfss://phelanrealtime@ayanadatalake.dfs.core.windows.net/phelan_realtime/2022/05/06/*\",format='csv',header = True)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df=  spark.read.load(location1, format='csv', header=True)\r\n",
					"df=  spark.read.load(\"abfss://phelanrealtime@ayanadatalake.dfs.core.windows.net/phelan_realtime/2022/04/08/*\", format='csv', header=True)\r\n",
					"df = spark.read.load(location1, format='csv', header=True)\r\n",
					"#display(df)\r\n",
					"df=df.toPandas()\r\n",
					"df"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#df.to_parquet('abfss://repono@ayanadatalake.dfs.core.windows.net/SCBs/SCBPhelan/name.parquet')"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd \r\n",
					"import datetime\r\n",
					"import numpy as np\r\n",
					"from pathlib import Path\r\n",
					"import warnings\r\n",
					"import sys\r\n",
					"warnings.filterwarnings('ignore')\r\n",
					"\r\n",
					"New=pd.DataFrame(columns=[])\r\n",
					"Block_df=pd.DataFrame(columns=[])\r\n",
					"Cloud=pd.DataFrame(columns=[])\r\n",
					"dataframe=pd.DataFrame(columns=[])\r\n",
					"dict0={'ICR08':'Antpr1','ICR10':'Antpr1','ICR12':'Antpr1','ICR13':'Antpr1','ICR16':'Antpr2',\r\n",
					"      'ICR17':'Antpr2','ICR18':'Antpr2','ICR19':'Antpr2','ICR01':'Antpr3','ICR02':'Antpr3',\r\n",
					"      'ICR03':'Antpr3','ICR04':'Antpr3','ICR11':'Antpr4','ICR14':'Antpr4','ICR15':'Antpr4',\r\n",
					"      'ICR20':'Antpr4','ICR05':'Antpr5','ICR06':'Antpr5','ICR07':'Antpr5','ICR09':'Antpr5'}\r\n",
					"dict1={'B01':'Phelan1'}\r\n",
					"block=['Phelan1']\r\n",
					"Block_Name=['Block1','Block2','Block3','Block4','Block5']\r\n",
					"SMB_Analysis=['Bhdl_SCBAnalysis']\r\n",
					"Output_File=['Bhdl_SCB_DailyRank']\r\n",
					"Output_File1=['Bhdl_SCB_DailyCurrent']\r\n",
					"Cloud_Output=['Bhdl_Cloud']\r\n",
					"Critic_Output=['Bhdl_CriticalSCBCons']\r\n",
					"indx=0   ### It is basically indicating the Block number i.e if it is a 2nd Block then indx value will  be '1'.\r\n",
					"\r\n",
					"location_input='abfss://repono@ayanadatalake.dfs.core.windows.net/Bhdl/Bhdl_StaticData/Bhdl_StaticData_SCB/'\r\n",
					"location_output='abfss://repono@ayanadatalake.dfs.core.windows.net/Bhdl/Bhdl_Insights/Bhdl_Insights_SCBFault/'\r\n",
					"\r\n",
					"\r\n",
					"tags=pd.read_excel(\"abfss://repono@ayanadatalake.dfs.core.windows.net/Bhdl/Bhdl_StaticData/Bhdl_StaticData_Inverter/Bhdl_OperationalTags.xlsx\",sheet_name=\"SCB\")\r\n",
					"\r\n",
					"tag=list(tags['Tags'].dropna())\r\n",
					"\r\n",
					"df=df[df['itemname'].isin(tag)] \r\n",
					"\r\n",
					"df[\"ISTtime\"]=df[\"ISTtime\"].astype(\"datetime64[ns]\")\r\n",
					"df['Time']= df['ISTtime'].dt.strftime(\"%H:%M\")\r\n",
					"df=df[(df['Time'] >='06:00') & (df['Time']<='18:30')]\r\n",
					"\r\n",
					"\r\n",
					"New['Date_And_Time'] = pd.to_datetime(df['ISTtime']).dt.strftime(\"%Y-%m-%d %H:%M\")\r\n",
					"New['Tags']=df['itemname']\r\n",
					"New['Value']=df['value']\r\n",
					"\r\n",
					"New=New.drop_duplicates(keep=\"last\",inplace=False)\r\n",
					"New = New.sort_values(by = 'Date_And_Time')\r\n",
					"New.reset_index(level=0, inplace=True)    \r\n",
					"del New['index']\r\n",
					"\r\n",
					"###############################################\r\n",
					"df2=New.copy()\r\n",
					"\r\n",
					"df2[\"Date_And_Time\"]=df2[\"Date_And_Time\"].astype(\"datetime64[ns]\")\r\n",
					"df2[\"Date\"]=pd.DatetimeIndex(df2[\"Date_And_Time\"]).date\r\n",
					"df2[\"Time\"]=pd.DatetimeIndex(df2[\"Date_And_Time\"]).time\r\n",
					"df2[\"Hour\"]=pd.DatetimeIndex(df2[\"Date_And_Time\"]).hour\r\n",
					"df2[\"Minute\"]=pd.DatetimeIndex(df2[\"Date_And_Time\"]).minute\r\n",
					"Date=df2.loc[3,\"Date\"]\r\n",
					"def grouped_min(value):\r\n",
					"    if value in range(1,6):\r\n",
					"        return 5\r\n",
					"    elif value in range(6,11):\r\n",
					"        return 10\r\n",
					"    elif value in range(11,16):\r\n",
					"        return 15\r\n",
					"    elif value in range(16,21):\r\n",
					"        return 20\r\n",
					"    elif value in range(21,26):\r\n",
					"        return 25\r\n",
					"    elif value in range(26,31):\r\n",
					"        return 30\r\n",
					"    elif value in range(31,36):\r\n",
					"        return 35\r\n",
					"    elif value in range(36,41):\r\n",
					"        return 40\r\n",
					"    elif value in range(41,46):\r\n",
					"        return 45\r\n",
					"    elif value in range(46,51):\r\n",
					"        return 50\r\n",
					"    elif value in range(51,56):\r\n",
					"        return 55\r\n",
					"    elif value in range(56,60) or value==0:\r\n",
					"        return 0\r\n",
					"\r\n",
					"df2[\"grouped_min\"]=df2[\"Minute\"].apply(grouped_min)\r\n",
					"df2[\"Value\"] = pd.to_numeric(df2[\"Value\"], downcast=\"float\")\r\n",
					"df2c=df2.groupby(by=[\"Tags\",\"Hour\",\"grouped_min\"]).agg(\"mean\").reset_index()\r\n",
					"df2c['Date']=Date\r\n",
					"df2c[\"SPP\"]=\"Phelan\"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"df2c[\"ICR\"]=df2c[\"Tags\"].apply(lambda x:x.split(\"_\")[0])\r\n",
					"df2c[\"INV\"]=df2c[\"Tags\"].apply(lambda x:x.split(\"_\")[1].split('.')[0])\r\n",
					"# df2c[\"SCB\"]=df2c[\"Tags\"].apply(lambda x:x.split(\"_\")[1].split('_')[1])\r\n",
					"df2c[\"SCB\"]=df2c.Tags.str.split('_',expand=True)[2]\r\n",
					"\r\n",
					"df2c[\"Block\"]='Phelan1'\r\n",
					"\r\n",
					"df2c=df2c.set_index(\"Date\")\r\n",
					"df2c['Hour']=df2c['Hour'].astype(str).str.zfill(2)\r\n",
					"df2c['grouped_min']=df2c['grouped_min'].astype(str).str.zfill(2)\r\n",
					"df2c['Time']=df2c['Hour']+':'+df2c['grouped_min']\r\n",
					"df2c=df2c[[\"Time\",\"Hour\",\"Minute\",\"grouped_min\",\"SPP\",\"Block\",\"ICR\",\"INV\",\"SCB\",\"Tags\",\"Value\"]]\r\n",
					"ICR_dict={\"ITC1\":\"ITC01\",\r\n",
					"         \"ITC2\":\"ITC02\",\r\n",
					"         \"ITC3\":\"ITC03\",\r\n",
					"         \"ITC4\":\"ITC04\",\r\n",
					"         \"ITC5\":\"ITC05\",\r\n",
					"         \"ITC6\":\"ITC06\",\r\n",
					"         \"ITC7\":\"ITC07\",\r\n",
					"         \"ITC8\":\"ITC08\",\r\n",
					"         \"ITC9\":\"ITC09\"}\r\n",
					"df2c[\"ICR\"].replace(ICR_dict,inplace=True)\r\n",
					"df2c.reset_index(level=0, inplace=True)   ### Resetting  Index \r\n",
					"df2c=df2c[df2c['Tags'].isin(tag)]\r\n",
					"New=df2c[[\"Date\",\"Time\",\"Tags\",\"Value\"]]\r\n",
					"New.to_excel(location_output+\"Bhdl_SCB_DailyRaw.xlsx\",index=False)\r\n",
					"\r\n",
					"#######################################################\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"# for con in tag:\r\n",
					"#     New1=New[New['Tags']==con]\r\n",
					"#     print(con)\r\n",
					"#     del New1['Tags']\r\n",
					"#     New1['Date_And_Time']=pd.to_datetime(New1['Date_And_Time'],errors='coerce')\r\n",
					"#     New1 = New1.sort_values(by = 'Date_And_Time')\r\n",
					"#     New1=New1.set_index('Date_And_Time',inplace=False)   # Converting the \"Data_And_Time\" column into index form, bcoz the math shouldn't apply for this column\r\n",
					"#     New1=New1.resample(\"5T\").mean()          # Converting the 1 minute data into 10 minute data by taking the average of the all 10 minutes data & placing it in a single cell as one data\r\n",
					"#     New1.reset_index(level=0, inplace=True)   \r\n",
					"#     New1['Tags']=con\r\n",
					"#     Frame=[dataframe,New1]\r\n",
					"#     dataframe=pd.concat(Frame,axis=0)\r\n",
					"# dataframe.reset_index(level=0, inplace=True)   ### Resetting  Index \r\n",
					"# del dataframe['index']   \r\n",
					"# New=dataframe\r\n",
					"# New1.to_excel(\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Man.xlsx\")\r\n",
					"\r\n",
					"# New['Date'] = pd.to_datetime(New['Date_And_Time']).dt.strftime(\"%Y-%m-%d\")   \r\n",
					"# New['Time'] = pd.to_datetime(New['Date_And_Time']).dt.strftime(\"%H:%M\")                                                               \r\n",
					"# New['Sitename']='Antpr'\r\n",
					"# df.reset_index(level=0, inplace=True)    \r\n",
					"# del df['index']                                           \r\n",
					"# Columns=New.Tags.str.split('.',expand=True)\r\n",
					"# Frame=[New,Columns]\r\n",
					"# New=pd.concat(Frame,axis=1)\r\n",
					"# del Columns\r\n",
					"# del Frame\r\n",
					"# New=New.rename(columns ={0:\"ICR\"})\r\n",
					"# New=New.rename(columns ={2:\"INV\"})  \r\n",
					"# New=New.rename(columns ={3:\"SCB\"})\r\n",
					"# New['Number']=New['ICR'].str.extract('(\\d+)')\r\n",
					"# New['Number']=New['Number'].str.zfill(2)\r\n",
					"# New['Number'] = New['Number'].replace(np.nan, '', regex=True)\r\n",
					"# New['Number'] = New['Number'].replace([None], [''], regex=True)\r\n",
					"# New['Char']=(New['ICR'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"# New['Char'] = New['Char'].replace(np.nan,'', regex=True)\r\n",
					"# New['Char'] = New['Char'].replace([None],[''], regex=True)\r\n",
					"# New['ICR']=New['Char']+New['Number']\r\n",
					"# New['Number']=New['INV'].str.extract('(\\d+)')\r\n",
					"# New['Number']=New['Number'].str.zfill(2)\r\n",
					"# New['Number'] = New['Number'].replace(np.nan, '', regex=True)\r\n",
					"# New['Number'] = New['Number'].replace([None],[''], regex=True)\r\n",
					"# New['Char']=(New['INV'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"# New['Char'] = New['Char'].replace(np.nan, '', regex=True)\r\n",
					"# New['Char'] = New['Char'].replace([None], [''], regex=True)\r\n",
					"# New['INV']=New['Char']+New['Number']\r\n",
					"# New['Number']=New['SCB'].str.extract('(\\d+)')\r\n",
					"# New['Number']=New['Number'].str.zfill(2)\r\n",
					"# New['Number'] = New['Number'].replace(np.nan, '', regex=True)\r\n",
					"# New['Number'] = New['Number'].replace([None], [''], regex=True)\r\n",
					"# New['Char']=(New['SCB'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"# New['Char'] = New['Char'].replace(np.nan, '', regex=True)\r\n",
					"# New['Char'] = New['Char'].replace([None], [''], regex=True)\r\n",
					"# New['SCB']=New['Char']+New['Number']\r\n",
					"# del New['Char'],New['Number']\r\n",
					"\r\n",
					"# New['ICR']=New['ICR'].replace([None],[''] , regex=True)\r\n",
					"# New['INV']=New['INV'].replace([''],[None] , regex=True)\r\n",
					"# New['SCB']=New['SCB'].replace([''],[None] , regex=True)\r\n",
					"\r\n",
					"# New['Block1'] = New['ICR'].map(dict0)\r\n",
					"# New['Block1'] = New['Block1'].replace(np.nan, '', regex=True)\r\n",
					"# New['Block2'] = New['INV'].map(dict1)\r\n",
					"# New['Block2'] = New['Block2'].replace(np.nan, '', regex=True)\r\n",
					"# New['Block']=New['Block1']+New['Block2']\r\n",
					"# del New['Block1'],New['Block2']\r\n",
					"\r\n",
					"# New['ICR'] = New['ICR'].replace(np.nan, '', regex=True)\r\n",
					"# New['ICR'] = New['ICR'].replace([None], [''], regex=True)\r\n",
					"# New['INV'] = New['INV'].replace(np.nan, '', regex=True)\r\n",
					"# New['INV'] = New['INV'].replace([None], [''], regex=True)\r\n",
					"# New['SCB'] = New['SCB'].replace(np.nan, '', regex=True)\r\n",
					"# New['SCB'] = New['SCB'].replace([None], [''], regex=True)\r\n",
					"# New['Block'] = New['Block'].replace(np.nan, '', regex=True)\r\n",
					"# New['Block'] = New['Block'].replace([None], [''], regex=True)\r\n",
					"\r\n",
					"\r\n",
					"# New = New.reindex(columns=['Date_And_Time','Date','Time','Sitename','Block','Tags','ICR','INV','SCB','Value'])\r\n",
					"# New.to_csv(location_output+\"SCB_Raw.csv\",index=False)\r\n",
					"# Temp_df=New\r\n",
					"# Columns=['Date','Time','Tags','Value']\r\n",
					"# New= pd.DataFrame(New, columns=Columns)\r\n",
					"Date=New.iloc[0,0]\r\n",
					"New_df1=New.pivot(\r\n",
					"    index='Time',    # Column to use to make new frame’s index. If None, uses existing index.\r\n",
					"    columns='Tags',  # Column to use to make new frame’s columns.\r\n",
					"    values='Value'    # Column(s) to use for populating new frame’s values.\r\n",
					")\r\n",
					"New_df1['Date']=Date\r\n",
					"tag.insert(0,'Date')\r\n",
					"New_df1 = New_df1.reindex(columns=tag)\r\n",
					"New_df1.reset_index(level=0, inplace=True)   ### Resetting  Index \r\n",
					"KPI=''\r\n",
					"Not_Working=[]\r\n",
					"print(block[indx],\"\\n\")\r\n",
					"SMB_Analysis_File=pd.read_csv(location_input+str(SMB_Analysis[indx])+\".csv\")\r\n",
					"cols=[]\r\n",
					"Block_df=pd.DataFrame(columns=cols)\r\n",
					"Block_df1=pd.DataFrame(columns=cols)\r\n",
					"Block_List=pd.DataFrame(tags[Block_Name[indx]].dropna())\r\n",
					"Block_List=Block_List.rename(columns ={Block_Name[indx]:\"Tags\"})\r\n",
					"Block_df= pd.DataFrame(New_df1, columns=Block_List['Tags'])\r\n",
					"Block_df=Block_df.rename(columns ={Block_List.loc[1,'Tags']:\"GIR\"})\r\n",
					"\r\n",
					"\r\n",
					"Block_df=Block_df.fillna(1)\r\n",
					"\r\n",
					"\r\n",
					"if Block_df['GIR'].isna().all(0):\r\n",
					"    print(\"\\nError : All the GIR Values are Empty! for Block 1\\n\")\r\n",
					"    sys.exit()\r\n",
					"if Block_df['GIR'].isna().any():\r\n",
					"    Block_df=Block_df['GIR'].fillna(0)\r\n",
					"Not_Working=(Block_df.columns[Block_df.isna().all(0)])\r\n",
					"Block_df=Block_df.drop(columns=Not_Working,axis=1)                 ### Dropping the unwanted columns\r\n",
					"check_for_nan = Block_df.loc[:, Block_df.isna().any()]\r\n",
					"check_for_nan=list(check_for_nan.columns)\r\n",
					"if 'GIR' in check_for_nan: \r\n",
					"    check_for_nan.remove('GIR')\r\n",
					"Block_df=Block_df.drop(columns=check_for_nan,axis=1)            ### Dropping the unwanted columns\r\n",
					"Block_df=Block_df.replace(np.nan, 10)\r\n",
					"Block_List=Block_List.drop([0,1],axis=0)            ### Dropping the unwanted columns\r\n",
					"index_number=Block_List[Block_List['Tags'].isin(Not_Working)].index\r\n",
					"Block_List=Block_List.drop(index_number,axis=0)            ### Dropping the unwanted rows\r\n",
					"Block_List.reset_index(level=0, inplace=True)\r\n",
					"del Block_List['index']\r\n",
					"\r\n",
					"index_number=Block_List[Block_List['Tags'].isin(check_for_nan)].index\r\n",
					"Block_List=Block_List.drop(index_number,axis=0)            ### Dropping the unwanted rows\r\n",
					"Block_List.reset_index(level=0, inplace=True)  \r\n",
					"del Block_List['index']\r\n",
					"\r\n",
					"index_number=SMB_Analysis_File[SMB_Analysis_File['Tags'].isin(Not_Working)].index\r\n",
					"mapping_df1=SMB_Analysis_File.loc[index_number]\r\n",
					"SMB_Analysis_File=SMB_Analysis_File.drop(index_number,axis=0)            ### Dropping the unwanted rows\r\n",
					"SMB_Analysis_File.reset_index(level=0, inplace=True)\r\n",
					"del SMB_Analysis_File['index']\r\n",
					"dict_Not_Working=dict(zip((mapping_df1['Tags']),(mapping_df1['FieldSCBName'])))\r\n",
					"\r\n",
					"index_number=SMB_Analysis_File[SMB_Analysis_File['Tags'].isin(check_for_nan)].index\r\n",
					"mapping_df2=SMB_Analysis_File.loc[index_number]\r\n",
					"SMB_Analysis_File=SMB_Analysis_File.drop(index_number,axis=0)            ### Dropping the unwanted rows\r\n",
					"SMB_Analysis_File.reset_index(level=0, inplace=True)\r\n",
					"del SMB_Analysis_File['index']\r\n",
					"dict_check_for_nan=dict(zip((mapping_df2['Tags']),(mapping_df2['FieldSCBName'])))\r\n",
					"\r\n",
					"\r\n",
					"Columns=Block_List.Tags.str.split('_',expand=True)\r\n",
					"Frame=[Block_df1,Columns]\r\n",
					"Block_df1=pd.concat(Frame,axis=1)\r\n",
					"if ((Block_df1.shape[0]>1) and (Block_df1.shape[1]>1)):\r\n",
					"    Block_df1=Block_df1.rename(columns ={0:\"ICR\"})\r\n",
					"    Block_df1=Block_df1.rename(columns ={1:\"INV\"})  \r\n",
					"    Block_df1=Block_df1.rename(columns ={2:\"SCB\"})\r\n",
					"    del Block_df1[3]\r\n",
					"  \r\n",
					"    Block_df1['Old_INV']=Block_df1['INV']\r\n",
					"    Block_df1['Number']=Block_df1['ICR'].str.extract('(\\d+)')\r\n",
					"    Block_df1['Number']=Block_df1['Number'].str.zfill(2)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['Char']=(Block_df1['ICR'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace(np.nan,'', regex=True)\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace([None],[''], regex=True)\r\n",
					"    Block_df1['ICR']=Block_df1['Char']+Block_df1['Number']\r\n",
					"    \r\n",
					"    Block_df1['INV']=Block_df1['ICR']+'_'+Block_df1['INV']\r\n",
					"    Block_df1['INV']=Block_df1['INV'].replace('INV', '', regex=True)\r\n",
					"    Block_df1['INV']=Block_df1['INV'].replace('ICR', 'I', regex=True)\r\n",
					"\r\n",
					"\r\n",
					"    Block_df1['Number']=Block_df1['Old_INV'].str.extract('(\\d+)')\r\n",
					"    Block_df1['Number']=Block_df1['Number'].str.zfill(2)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace([None],[''], regex=True)\r\n",
					"    Block_df1['Char']=(Block_df1['Old_INV'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['INV1']=Block_df1['Char']+Block_df1['Number']\r\n",
					"    \r\n",
					"    Block_df1['Number']=Block_df1['SCB'].str.extract('(\\d+)')\r\n",
					"    Block_df1['Number']=Block_df1['Number'].str.zfill(2)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Number'] = Block_df1['Number'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['Char']=(Block_df1['SCB'].str.extract(r\"([A-Za-z_.]+)\"))\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['Char'] = Block_df1['Char'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['SCB']=Block_df1['Char']+Block_df1['Number']\r\n",
					"    del Block_df1['Char'],Block_df1['Number']\r\n",
					"    \r\n",
					"    Block_df1['ICR']=Block_df1['ICR'].replace([None],[''] , regex=True)\r\n",
					"    Block_df1['INV']=Block_df1['INV'].replace([''],[None] , regex=True)\r\n",
					"    Block_df1['SCB']=Block_df1['SCB'].replace([''],[None] , regex=True)\r\n",
					"    \r\n",
					"    \r\n",
					"    Block_df1['ICR'] = Block_df1['ICR'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['ICR'] = Block_df1['ICR'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['INV'] = Block_df1['INV'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['INV'] = Block_df1['INV'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['SCB'] = Block_df1['SCB'].replace(np.nan, '', regex=True)\r\n",
					"    Block_df1['SCB'] = Block_df1['SCB'].replace([None], [''], regex=True)\r\n",
					"    Block_df1['Block'] = 'Phelan1'\r\n",
					"  \r\n",
					"    Block_df1['Date']=Date\r\n",
					"    Icr_Inv_SCB=list(Block_df1['ICR']+'..'+Block_df1['INV1']+'.'+Block_df1['SCB'])\r\n",
					"    del Block_df1['INV1'],Block_df1['Old_INV']\r\n",
					"    \r\n",
					"    Old_Coulmn_Name=Block_df.columns\r\n",
					"    Appending_List=['Date','GIR']\r\n",
					"    Icr_Inv_SCB_1=Appending_List+Icr_Inv_SCB\r\n",
					"    Block_df.rename(columns={i:j for i,j in zip(Old_Coulmn_Name,Icr_Inv_SCB_1)}, inplace=True)\r\n",
					"    Block_df = Block_df.sort_index(ascending=True, axis=1)\r\n",
					"    Time_List=New_df1['Time']\r\n",
					"    New_Coulmn=['Date','Block','ICR','INV','SCB','Block_Inverter_SCB','Total_SCB_Current',\r\n",
					"                'Deviation_wrt_R1_Block_df1','Eff_String','Total_String_Current','Deviation_All',\r\n",
					"                'OverAll_Block_df1_Eff_Rank','Ideal_Current','Current_At_Max_GIR','Ideal_Current_At_Max_GIR',\r\n",
					"                'Delta','Rank_Delta','Rank_Day_Peak','Rank_Ranked_Day_Peak']\r\n",
					"    Block_df1 = Block_df1.reindex(columns=New_Coulmn)\r\n",
					"    Block_df.insert(1, \"Time\", Time_List)\r\n",
					"    Block_df1['Block_Inverter_SCB']=Icr_Inv_SCB\r\n",
					"    Block_df1 = Block_df1.sort_values(by = 'Block_Inverter_SCB')\r\n",
					"    index_number=Block_df1[Block_df1['Block_Inverter_SCB'].isin(Not_Working)].index\r\n",
					"    Block_df1=Block_df1.drop(index_number,axis=0)          ### Dropping the unwanted columns\r\n",
					"    Block_df1.reset_index(level=0, inplace=True)  \r\n",
					"    del Block_df1['index']\r\n",
					"    index_number=Block_df1[Block_df1['Block_Inverter_SCB'].isin(check_for_nan)].index\r\n",
					"    Block_df1=Block_df1.drop(index_number,axis=0)          ### Dropping the unwanted columns\r\n",
					"    Block_df1.reset_index(level=0, inplace=True)  \r\n",
					"    del Block_df1['index']\r\n",
					"    GIR_List=list(Block_df['GIR'])\r\n",
					"    Sum=list(Block_df.sum())\r\n",
					"    Sum.pop(0)      ### Popping the sum of the column Date,Time and GIR value from total Sum\r\n",
					"    Sum.pop(0)\r\n",
					"    Block_df1['Total_SCB_Current']=Sum\r\n",
					"    Block_df1['Deviation_wrt_R1_Block_df1']=(Block_df1['Total_SCB_Current']-(Block_df1['Total_SCB_Current'].max()))/(Block_df1['Total_SCB_Current'].max())\r\n",
					"    Curr_Per_String=Sum/SMB_Analysis_File['Effective_Strings']       \r\n",
					"    a = np.array(GIR_List)\r\n",
					"    Max_Value_Index = np.argmax(a)\r\n",
					"    Max_Value=GIR_List[Max_Value_Index]\r\n",
					"    def calculate_rank(Curr_Per_String):                                    ### This \"for\" loop is used to calculate the rank based on Curr_Per_String & stores it in a list & returns\r\n",
					"        a={}\r\n",
					"        rank=1\r\n",
					"        for num in sorted(Curr_Per_String, reverse=True):\r\n",
					"            if num not in a:\r\n",
					"                a[num]=rank\r\n",
					"                rank=rank+1\r\n",
					"        return[a[i] for i in  Curr_Per_String]\r\n",
					"    Rank1=calculate_rank(Curr_Per_String)   \r\n",
					"    Block_df1['Eff_String']=SMB_Analysis_File['Effective_Strings']\r\n",
					"    Block_df1[\"Total_String_Current\"]=Block_df1['Total_SCB_Current']/Block_df1['Eff_String']\r\n",
					"    Block_df1['Deviation_All']=(Block_df1[\"Total_String_Current\"]-Block_df1[\"Total_String_Current\"].max())/Block_df1[\"Total_String_Current\"].max()\r\n",
					"    Block_df1['OverAll_Block_df1_Eff_Rank']=Rank1\r\n",
					"    Block_df1['Ideal_Current']=SMB_Analysis_File['Ideal_Current']\r\n",
					"    Block_df1['Current_At_Max_GIR']=list(Block_df.iloc[Max_Value_Index,3:])\r\n",
					"    Block_df1['Ideal_Current_At_Max_GIR']=(Block_df1['Ideal_Current']/1000)*Max_Value\r\n",
					"    Block_df1['Delta']=Block_df1['Current_At_Max_GIR']-Block_df1['Ideal_Current_At_Max_GIR']\r\n",
					"    Block_df1['Rank_Delta']=Block_df1.groupby('Date')['Delta'].rank(ascending=False)\r\n",
					"    Block_df1['Rank_Day_Peak']=Block_df1['Rank_Delta']*Block_df1['OverAll_Block_df1_Eff_Rank']   \r\n",
					"    \r\n",
					"    Rank2=[]\r\n",
					"    rank_list=Block_df1['Rank_Day_Peak']\r\n",
					"    def calculate_rank1(rank_list): ### This \"for\" loop is used to calculate the rank based on sum_of_cols & stores it in a list & returns\r\n",
					"        a={}\r\n",
					"        rank=1\r\n",
					"        for num in sorted(rank_list, reverse=False):\r\n",
					"            if num not in a:\r\n",
					"                a[num]=rank\r\n",
					"                rank=rank+1\r\n",
					"        return[a[i] for i in  rank_list]\r\n",
					"    Rank2=calculate_rank1(rank_list)\r\n",
					"    Block_df1['Rank_Ranked_Day_Peak']=Rank2\r\n",
					"    Block_df1.to_excel(location_output+str(Output_File[indx])+\".xlsx\",index=False)\r\n",
					"    Block_df.to_excel(location_output+str(Output_File1[indx])+\".xlsx\",index=False)\r\n",
					"\r\n",
					"    Dup=Block_df1\r\n",
					"    Dup.sort_values(\"Rank_Ranked_Day_Peak\", axis = 0, ascending = False,inplace = True)\r\n",
					"    Dup.reset_index(level=0, inplace=True)\r\n",
					"    Comparision=pd.read_excel(location_input+\"Bhdl_Check.xlsx\",sheet_name=0,header=0)\r\n",
					"    Comparision_Per=(Comparision.loc[0,'Percentage_of_Critical_SMB'])/100\r\n",
					"    High_Value=Dup.loc[0,'Rank_Ranked_Day_Peak']\r\n",
					"    Low_Value=Dup.loc[Dup['Rank_Ranked_Day_Peak'].count()-1,'Rank_Ranked_Day_Peak']\r\n",
					"    RESULT=High_Value-Low_Value\r\n",
					"    RESULT=RESULT*Comparision_Per\r\n",
					"    Final_Val=High_Value-RESULT\r\n",
					"    \r\n",
					"    Lossing=[]\r\n",
					"    Loss_str=[]\r\n",
					"    for g in range(Block_df1['Rank_Ranked_Day_Peak'].count()):\r\n",
					"        if Block_df1.loc[g,'Rank_Ranked_Day_Peak']>Final_Val:\r\n",
					"            Lossing.append(Block_df1.loc[g,'Block_Inverter_SCB'])\r\n",
					"            Loss_str+=str(Block_df1.loc[g,'Block_Inverter_SCB'])+'\\n\\n'\r\n",
					"    if Path(location_output+str(Critic_Output[indx])+'.xlsx').is_file():\r\n",
					"        Critic=pd.read_excel(location_output+str(Critic_Output[indx])+\".xlsx\")         ### If File is present, reading the that file into a dataframe\r\n",
					"    else:\r\n",
					"        cols=['Date','Block_Inverter_SMB','FieldSCBName']\r\n",
					"        Critic= pd.DataFrame(columns = cols)                             ### If File is not present, Create a new dataframe by above columns\r\n",
					"    \r\n",
					"    index_number=SMB_Analysis_File[SMB_Analysis_File['Tags_Acc_Output'].isin(Lossing)].index\r\n",
					"    mapping_df2=SMB_Analysis_File.loc[index_number]\r\n",
					"    dict_Lossing=dict(zip((mapping_df2['Tags_Acc_Output']),(mapping_df2['Block_Inverter_SMB'])))\r\n",
					"    Lossing1 = list([dict_Lossing[key] for key in Lossing])\r\n",
					"    \r\n",
					"    for c in range(len(Lossing)):\r\n",
					"        Critic.loc[len(Critic)]=[Date,Lossing[c],Lossing1[c]]\r\n",
					"    Critic.to_excel(location_output+str(Critic_Output[indx])+'.xlsx',index=False)\r\n",
					"\r\n",
					"    \r\n",
					"                            ####################### Cloud Computing #########################\r\n",
					"    Rad=[]\r\n",
					"    Rad1=[]\r\n",
					"    Std_gir=[]\r\n",
					"    for j in range(Block_df['GIR'].count()):\r\n",
					"        Rad.append(Block_df.loc[j,'GIR'])\r\n",
					"        if(Rad[j]==0):\r\n",
					"            Rad[j]=1\r\n",
					"    row_no1=0\r\n",
					"    row_no2=0\r\n",
					"    for i in range(len(Rad)):\r\n",
					"        if Rad[i]>=820:\r\n",
					"            row_no1=i\r\n",
					"            break;\r\n",
					"    for i in reversed(Rad):\r\n",
					"        Rad1.append(i)\r\n",
					"    for i in range(len(Rad1)):\r\n",
					"        if Rad1[i]>=820:\r\n",
					"            row_no2=(len(Rad1)-i)-2\r\n",
					"            break;\r\n",
					"    Cloud['Date']=Block_df['Date']\r\n",
					"    Cloud['Time']=Block_df['Time']\r\n",
					"    Cloud['GIR']=Block_df['GIR']\r\n",
					"    for ind in range(Block_df['GIR'].count()):\r\n",
					"        Std_gir.append((Block_df.loc[ind,'GIR']-Block_df['GIR'].mean())/Block_df['GIR'].std())\r\n",
					"    inv1=1\r\n",
					"    blk1=1\r\n",
					"    smb1=1\r\n",
					"    qwer=0\r\n",
					"    cld=[0]\r\n",
					"    for index in range(len(Block_df.count())):   \r\n",
					"        if(index>2):\r\n",
					"            qwer+=1\r\n",
					"            std_curr=[]\r\n",
					"            curr=[]\r\n",
					"            Rad1=[]\r\n",
					"            curr1=[]\r\n",
					"            Rad2=[]\r\n",
					"            curr2=[]\r\n",
					"            Delta=[]\r\n",
					"            cld1=[]\r\n",
					"            cld2=[]\r\n",
					"            cld3=[]\r\n",
					"            cld4=[]\r\n",
					"            cld5=[]  \r\n",
					"            for oi in range(Block_df['GIR'].count()):    \r\n",
					"                std_curr.append((Block_df.iloc[oi,index]-Block_df.iloc[:,index].mean())/Block_df.iloc[:,index].std())\r\n",
					"                curr.append(Block_df.iloc[oi,index])\r\n",
					"                if(curr[oi]==0):\r\n",
					"                    curr[oi]=1\r\n",
					"            for io in range(len(Rad)-1):\r\n",
					"                Rad1.append(((Rad[io+1]-Rad[io])/Rad[io+1])*100)\r\n",
					"                curr1.append(((curr[io+1]-curr[io])/curr[io+1])*100)\r\n",
					"                Delta.append(curr1[io]-Rad1[io])\r\n",
					"            for io in range(len(Delta)):\r\n",
					"                if Delta[io]<Comparision.loc[0,'Percentage_of_Cloud_Difference']: \r\n",
					"                    cld1.append(1)\r\n",
					"                else:\r\n",
					"                    cld1.append(0)\r\n",
					"                Rad2.append((Rad[io]-Rad[io+1]))\r\n",
					"                curr2.append((curr[io]-curr[io+1]))\r\n",
					"            if row_no2!=0:\r\n",
					"                for iop in range(row_no2+1):\r\n",
					"                    if(Rad2[iop]>0 and curr2[iop]>0): \r\n",
					"                        cld2.append(1)\r\n",
					"                    else:\r\n",
					"                        cld2.append(0)\r\n",
					"                for poi in range(len(Rad2)):\r\n",
					"                    if poi>row_no2:\r\n",
					"                        if(Rad2[poi]<0 and curr2[poi]<0):\r\n",
					"                            cld2.append(1)\r\n",
					"                        else:\r\n",
					"                            cld2.append(0)\r\n",
					"            else:\r\n",
					"                 for iop in range(len(Rad2)):\r\n",
					"                    if(Rad2[iop]>0 and curr2[iop]>0): \r\n",
					"                        cld2.append(1)\r\n",
					"                    else:\r\n",
					"                        cld2.append(0)\r\n",
					"            for uio in range(len(cld1)):\r\n",
					"                if(cld1[uio]==1 or cld2[uio]==1):\r\n",
					"                    cld3.append(1)\r\n",
					"                else:\r\n",
					"                    cld3.append(0)\r\n",
					"            if row_no2!=0:\r\n",
					"                for oiu in range(len(cld3)):\r\n",
					"                    if oiu>=row_no1 and oiu<=row_no2:\r\n",
					"                        if Std_gir[oiu]<Std_gir[row_no1] and std_curr[oiu]<std_curr[row_no1]:\r\n",
					"                            cld4.append(1)\r\n",
					"                        else:\r\n",
					"                            cld4.append(0)\r\n",
					"                    else:\r\n",
					"                        cld4.append(0)               \r\n",
					"                cld4.pop(0)\r\n",
					"                cld4=cld4+cld\r\n",
					"            if row_no2!=0:           \r\n",
					"                for iu in range(len(cld3)):\r\n",
					"                    if(cld3[iu]==1 or cld4[iu]==1):\r\n",
					"                        cld5.append(1)\r\n",
					"                    else:\r\n",
					"                        cld5.append(0)\r\n",
					"            else:\r\n",
					"                cld5=cld3\r\n",
					"            str123=Block_df.columns[index]\r\n",
					"            cld5=cld+cld5\r\n",
					"            Cloud[str123]=cld5\r\n",
					"    Cloud.to_excel(location_output+str(Cloud_Output[indx])+\".xlsx\",index=False)\r\n",
					"    Min_row=Max_Value_Index-6\r\n",
					"    Max_row=Max_Value_Index+6\r\n",
					"    Sum1=[]\r\n",
					"    for c in range(len(Lossing)):\r\n",
					"        Total=0\r\n",
					"        for ind in range(Min_row,Max_row+1):\r\n",
					"            Total+=Cloud.loc[ind,Lossing[c]]\r\n",
					"        Sum1.append(Total)\r\n",
					"    a = np.array(Sum1)\r\n",
					"    Min_Value_Index = np.argmin(a)\r\n",
					"    Min_Value=Sum1[Min_Value_Index]\r\n",
					"    \r\n",
					"    index_number=SMB_Analysis_File[SMB_Analysis_File['Tags_Acc_Output'].isin(Lossing)].index\r\n",
					"    mapping_df2=SMB_Analysis_File.loc[index_number]\r\n",
					"    dict_Lossing=dict(zip((mapping_df2['Tags_Acc_Output']),(mapping_df2['FieldSCBName'])))\r\n",
					"    Lossing = list([dict_Lossing[key] for key in Lossing])\r\n",
					"\r\n",
					"    Loss_Str=''\r\n",
					"    for i in range(len(Lossing)):\r\n",
					"        if i==0 and Min_Value_Index==0:\r\n",
					"            Loss_Str+=str(Lossing[i])+u\"\\u26A0\"+''+u\"\\u26C5\"+\"\\n\\n\"\r\n",
					"        elif i==Min_Value_Index:\r\n",
					"            Loss_Str+=str(Lossing[i])+u\"\\u26C5\"+\"\\n\\n\"\r\n",
					"        elif i==0:\r\n",
					"            Loss_Str+=str(Lossing[i])+u\"\\u26A0\"+\"\\n\\n\"                                         \r\n",
					"        else:\r\n",
					"            Loss_Str+=str(Lossing[i])+\"\\n\\n\"\r\n",
					"    # print(Loss_Str)\r\n",
					"    \r\n",
					"\r\n",
					"\r\n",
					"    comm_error=''\r\n",
					"    if len(Not_Working)>0 or len(check_for_nan)>0 :\r\n",
					"        Not_Working = list([dict_Not_Working[key] for key in Not_Working])\r\n",
					"        check_for_nan = list([dict_check_for_nan[key] for key in check_for_nan])\r\n",
					"        comm_error='Communication Error:\\n\\n'\r\n",
					"        for comm in range(len(Not_Working)):\r\n",
					"            comm_error+=''.join(Not_Working[comm])+'\\n\\n'\r\n",
					"        for comm in range(len(check_for_nan)):\r\n",
					"            comm_error+=''.join(check_for_nan[comm])+'  !!\\n\\n'\r\n",
					"        KPI='***'+block[indx]+': '+str(Date)+'***\\n\\n'+comm_error+'Faulty SMBs:\\n\\n'+Loss_Str+'\\n\\n'\r\n",
					"    else:\r\n",
					"        KPI='**'+block[indx]+': '+str(Date)+'**\\n\\nFaulty SMBs:\\n\\n'+Loss_Str+'\\n\\n'\r\n",
					"    def sms():\r\n",
					"        import pymsteams\r\n",
					"        myTeamsMessage = pymsteams.connectorcard(\"https://ayanapower.webhook.office.com/webhookb2/17435f69-7867-4a41-b5f0-46ceb0fb534f@59b60474-e282-44b5-881c-bb9ce815690c/IncomingWebhook/2bd33d772a4e43018defac31ef69ed0a/0a64ab19-2984-4746-9447-21882231bb32\")\r\n",
					"        myTeamsMessage.text(KPI)\r\n",
					"        myTeamsMessage.send()\r\n",
					"    sms()\r\n",
					"    print(KPI)\r\n",
					"else:\r\n",
					"    # \"Communication Error\"\r\n",
					"    KPI='**'+block[indx]+': '+str(Date)+'**\\n\\n'+'Communication Error\\n\\n'\r\n",
					"    def sms():\r\n",
					"        import pymsteams\r\n",
					"        myTeamsMessage = pymsteams.connectorcard(\"https://ayanapower.webhook.office.com/webhookb2/17435f69-7867-4a41-b5f0-46ceb0fb534f@59b60474-e282-44b5-881c-bb9ce815690c/IncomingWebhook/d0276b722a2a453daf3af8d5f8564769/0a64ab19-2984-4746-9447-21882231bb32\")\r\n",
					"        myTeamsMessage.text(KPI)\r\n",
					"        myTeamsMessage.send()\r\n",
					"    sms()\r\n",
					"    print(KPI)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_spark2=spark.createDataFrame(Block_df1)\r\n",
					"spark.sql(\"create database if not exists phelan\")\r\n",
					"df_spark2.write.format(\"delta\").mode(\"append\").saveAsTable(\"phelan.scbperf\")"
				],
				"execution_count": 6
			}
		]
	}
}